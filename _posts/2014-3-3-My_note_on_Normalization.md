---
layout: post
title: Normalization in neuroscience and deep learning
---

Divisive Normalization (or gain control) has been suggested as a 'canonical computation' in the brain.  crucial computational unit for explaning the neuronal responses to different visual stimuli. Several response properties of neurons in different visual areas could only be explained after including normalization in the models. ([EXAMPLE from MT, V1, etc, Heeger]). 

In an apparently independent research stream, deep learning researchers came up with different types of activation normalizations (from 2015 with batch normalization [REF]) for facilitating deep net training process and learning better representations in convolutional and recurrent neural nets. 

How are different normalization techniques compare?

The comparison has been done here apparently: https://arxiv.org/pdf/1611.04520.pdf

